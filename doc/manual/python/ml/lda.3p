.\" Automatically generated by Pandoc 2.17.1.1
.\"
.\" Define V font for inline verbatim, using C font in formats
.\" that render this, and otherwise B font.
.ie "\f[CB]x\f[]"x" \{\
. ftr V B
. ftr VI BI
. ftr VB B
. ftr VBI BI
.\}
.el \{\
. ftr V CR
. ftr VI CI
. ftr VB CB
. ftr VBI CBI
.\}
.TH "Latent Dirichlet Allocation" "" "" "" ""
.hy
.SH NAME
.PP
Latent Dirichlet Allocation(LDA) - It is a Natural Language Processing
algorithm that allows a set of observations to be explained by
unobserved groups.
These are used to explain why some parts of the data are similar.
.SH SYNOPSIS
.IP
.nf
\f[C]
class frovedis.mllib.decomposition.LatentDirichletAllocation(n_components=10, 
                                                             doc_topic_prior=None, 
                                                             topic_word_prior=None,
                                                             learning_method=\[aq]batch\[aq],
                                                             learning_decay=.7, 
                                                             learning_offset=10.,
                                                             max_iter=10, batch_size=128,
                                                             evaluate_every=-1, 
                                                             total_samples=1e6,
                                                             perp_tol=1e-1, 
                                                             mean_change_tol=1e-3,
                                                             max_doc_update_iter=100,
                                                             n_jobs=None, verbose=0,
                                                             random_state=None, 
                                                             algorithm=\[dq]original\[dq],
                                                             explore_iter=0)  
\f[R]
.fi
.SS Public Member Functions
.PP
fit(X, y = None)
.PD 0
.P
.PD
transform(X)
.PD 0
.P
.PD
perplexity(X, sub_sampling = False)
.PD 0
.P
.PD
score(X, y = None)
.PD 0
.P
.PD
get_params(deep = True)
.PD 0
.P
.PD
set_params(**params)
.PD 0
.P
.PD
fit_transform(X, y = None)
.PD 0
.P
.PD
load(fname, dtype = None)
.PD 0
.P
.PD
save(fname)
.PD 0
.P
.PD
release()
.PD 0
.P
.PD
is_fitted()
.SH DESCRIPTION
.PP
Latent Dirichlet Allocation (LDA) is a widely used machine learning
technique for topic modeling.
The input is a corpus of documents and the output is per-document topic
distribution and per-topic word distribution.
.PP
This implementation provides python wrapper interface to Vectorised LDA
(VLDA), which accelerates LDA training by exploiting the data-level, and
the thread-level parallelism using vector processors.
The priority-aware scheduling approach is proposed to address the high
memory requirement and workload imbalance issues with existing works.
.PP
This module provides a client-server implementation, where the client
application is a normal python program.
The frovedis interface is almost same as Scikit-learn
LatentDirichletAllocation interface, but it doesn\[cq]t have any
dependency with Scikit-learn.
It can be used simply even if the system doesn\[cq]t have Scikit-learn
installed.
Thus in this implementation, a python client can interact with a
frovedis server sending the required python data for training at
frovedis side.
Python data is converted into frovedis compatible data internally and
the python ML call is linked with the respective frovedis ML call to get
the job done at frovedis server.
.PP
Python side calls for LatentDirichletAllocation on the frovedis server.
Once the training is completed with the input data at the frovedis
server, it returns an abstract model with a unique model ID to the
client python program.
.PP
When transform-like request would be made on the trained model, python
program will send the same request to the frovedis server.
After the request is served at the frovedis server, the output would be
sent back to the python client.
.SS Detailed Description
.SS 1. LatentDirichletAllocation()
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]n_components\f[B]\f[R]: A positive integer parameter
specifying the number of topics.
(Default: 10)
.PD 0
.P
.PD
\f[B]\f[BI]doc_topic_prior\f[B]\f[R]: A float parameter which specifies
the prior of document topic distribution theta.
It is also called alpha.
(Default: None)
.PD 0
.P
.PD
If it is None (not specified explicitly), it will be set as (1 /
n_components).
.PD 0
.P
.PD
\f[B]\f[BI]topic_word_prior\f[B]\f[R]: A float parameter which specifies
the prior of topic word distribution beta.
It is also called beta.
(Default: None)
.PD 0
.P
.PD
If it is None (not specified explicitly), it will be set as (1 /
n_components).
.PD 0
.P
.PD
\f[B]\f[BI]learning_method\f[B]\f[R]: An unused parameter which
specifies the method used to update `components_'.
.PD 0
.P
.PD
Although, this parameter is unused in frovedis but it must be `batch' or
`online' update.
This is simply done to keep the behavior consistent with Scikit-learn.
(Default: batch)
.PD 0
.P
.PD
\f[B]\f[BI]learning_decay\f[B]\f[R]: An unused parameter.
(Default: 0.7)
.PD 0
.P
.PD
\f[B]\f[BI]learning_offset\f[B]\f[R]: An unused parameter.
Although, this parameter is unused in frovedis but it must be zero or a
positive float value.
This is simply done to keep the behavior consistent with Scikit-learn.
(Default: 10.0)
.PD 0
.P
.PD
\f[B]\f[BI]max_iter\f[B]\f[R]: An integer parameter which specifies the
maximum number of passes over the training data.
(Default: 10)
.PD 0
.P
.PD
\f[B]\f[BI]batch_size\f[B]\f[R]: An unused parameter.
(Default: 128)
.PD 0
.P
.PD
\f[B]\f[BI]evaluate_every\f[B]\f[R]: An integer parameter that specifies
how often to evaluate perplexity.
Perplexity is not evaluated in training by default.
If this parameter is greater than 0, perplexity will be evaluated, which
can help in checking convergence in training process, but total training
time will be increased.
(Default: -1)
.PP
For example, when evaluate_every < 0
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
corpus = [
\[aq]This is the first document.\[aq],
\[aq]This document is the second document.\[aq],
\[aq]And this is the third one.\[aq],
\[aq]Is this the first document?\[aq],
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(corpus)

# fitting input matrix on lda object  
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3, evaluate_every = -1)
lda.fit(csr_mat)
\f[R]
.fi
.PP
\f[B]LDA training time: 0.0007402896881103516\f[R]
.PP
For example, when evaluate_every > 0
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
corpus = [
\[aq]This is the first document.\[aq],
\[aq]This document is the second document.\[aq],
\[aq]And this is the third one.\[aq],
\[aq]Is this the first document?\[aq],
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(corpus)

# fitting input matrix on lda object  
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3, evaluate_every = 2)
lda.fit(csr_mat)
\f[R]
.fi
.PP
\f[B]LDA training time: 0.0008301734924316406\f[R]
.PP
\f[B]\f[BI]total_samples\f[B]\f[R]: A positive integer parameter which
is unused in frovedis.
This is simply kept to make the behavior consistent with Scikit-learn.
(Default: 1e6)
.PD 0
.P
.PD
\f[B]\f[BI]perp_tol\f[B]\f[R]: An unused parameter.
(Default: 1e-1)
.PD 0
.P
.PD
\f[B]\f[BI]mean_change_tol\f[B]\f[R]: An unused parameter.
(Default: 1e-3)
.PD 0
.P
.PD
\f[B]\f[BI]max_doc_update_iter\f[B]\f[R]: An unused parameter.
(Default: 100)
.PD 0
.P
.PD
\f[B]\f[BI]n_jobs\f[B]\f[R]: An unused parameter.
(Default: None)
.PD 0
.P
.PD
\f[B]\f[BI]verbose\f[B]\f[R]: An integer parameter specifying the log
level to use.
Its value is set as 0 by default (for INFO mode).
But it can be set to 1(for DEBUG mode) or 2(for TRACE mode) for getting
training time logs from frovedis server.
.PD 0
.P
.PD
\f[B]\f[BI]algorithm\f[B]\f[R]: A string object parameter which
specifies the sampling technique to be used.
(Default: `original')
.PD 0
.P
.PD
Frovedis LatentDirichletAllocation(LDA) supports either of two sampling
techniques:
.PD 0
.P
.PD
\f[B]1.
Collapsed Gibbs Sampling (CGS)\f[R]
.PD 0
.P
.PD
\f[B]2.
Metropolis Hastings\f[R]
.PD 0
.P
.PD
The default sampling algorithm is CGS (Default: `original')
.PD 0
.P
.PD
If Metropolis Hastings is to be used, then it is required to set the
proposal types namely:
.PD 0
.P
.PD
\ \ \f[B]2a.
document proposal: \[lq]dp\[rq] algo,\f[R]
.PD 0
.P
.PD
\ \ \f[B]2b.
word proposal: \[lq]wp\[rq] algo,\f[R]
.PD 0
.P
.PD
\ \ \f[B]2c.
cycle proposal: \[lq]cp\[rq] algo,\f[R]
.PD 0
.P
.PD
\ \ \f[B]2d.
sparse lda: \[lq]sparse\[rq]\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]explore_iter\f[B]\f[R]: An integer parameter that specifies
the number of iterations to explore optimal hyperparams.
(Default: 0)
.PP
\f[B]Attributes\f[R]
.PD 0
.P
.PD
\f[B]components_\f[R]: A numpy ndarray of double (float64) type values
and has shape \f[B](n_components, n_features)\f[R].
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It initializes a LatentDirichletAllocation object with the given
parameters.
.PP
The parameters: \[lq]learning_method\[rq], \[lq]learning_decay\[rq],
\[lq]learning_offset\[rq], \[lq]batch_size\[rq],
\[lq]total_samples\[rq], \[lq]perp_tol\[rq], \[lq]mean_change_tol\[rq],
\[lq]max_doc_update_iter\[rq] and \[lq]n_jobs\[rq] are simply kept in to
make the interface uniform to the Scikit-learn LatentDirichletAllocation
module.
They are not used anywhere within the frovedis implementation.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 2. fit(X, y = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A scipy sparse matrix or an instance of
FrovedisCRSMatrix for sparse data of int, float or double (float64)
type.
It has shape \f[B](n_samples, n_features)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]y\f[B]\f[R]: None or any python array-like object (any
shape).
It is simply ignored in frovedis implementation, like in Scikit-learn.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
Fit LatentDirichletAllocation model on training data X.
.PP
For example,
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
corpus = [
\[aq]This is the first document.\[aq],
\[aq]This document is the second document.\[aq],
\[aq]And this is the third one.\[aq],
\[aq]Is this the first document?\[aq],
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(corpus)

# fitting input matrix on lda object  
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3)
lda.fit(csr_mat)
\f[R]
.fi
.PP
When native python data is provided, it is converted to frovedis-like
inputs and sent to frovedis server which consumes some data transfer
time.
Pre-constructed frovedis-like inputs can be used to speed up the
training time, especially when same data would be used for multiple
executions.
.PP
For example,
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
corpus = [
\[aq]This is the first document.\[aq],
\[aq]This document is the second document.\[aq],
\[aq]And this is the third one.\[aq],
\[aq]Is this the first document?\[aq],
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(corpus)

# Since \[dq]csr_mat\[dq] is scipy sparse data, we have created FrovedisCRSMatrix 
# Also it only supports int64 as itype for input sparse data during training
from frovedis.matrix.crs import FrovedisCRSMatrix
cmat = FrovedisCRSMatrix(csr_mat, dtype = np.int32, itype = np.int64)

# fitting input matrix on lda object  
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3)
lda.fit(cmat)  
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 3. transform(X)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A scipy sparse matrix or an instance of
FrovedisCRSMatrix for sparse data of int, float or double (float64)
type.
It has shape \f[B](n_samples, n_features)\f[R].
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It transforms input matrix X according to the trained model.
.PP
For example,
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
test_corpus = [
\[aq]This is the first second third document.\[aq],
\[aq]This This one third document.\[aq]
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.    
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(test_corpus)

# transform input according to trained model 
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3)
lda.fit(csr_mat)
print(lda.transform(csr_mat))
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
[[0.47619048 0.19047619 0.47619048]
 [0.26666667 0.26666667 0.66666667]]
\f[R]
.fi
.PP
Like in fit(), we can also provide frovedis-like input in transform()
for faster computation.
.PP
For example,
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
test_corpus = [
\[aq]This is the first second third document.\[aq],
\[aq]This This one third document.\[aq]
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(test_corpus)

# Since \[dq]csr_mat\[dq] is scipy sparse data, we have created FrovedisCRSMatrix 
# Also it only supports int64 as itype for input sparse data during training
from frovedis.matrix.crs import FrovedisCRSMatrix
cmat = FrovedisCRSMatrix(csr_mat, dtype = np.int32, itype = np.int64)

# transform input according to trained model 
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3)
lda.fit(cmat)
lda.transform(cmat).debug_print()
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
matrix:
num_row = 2, num_col = 3
node 0
node = 0, local_num_row = 2, local_num_col = 3, val = 0.47619 0.333333 0.333333 0.266667 
0.0666667 0.866667
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
- \f[B]When X is python native input:\f[R]
.PD 0
.P
.PD
It returns a python ndarray of shape \f[B](n_samples, n_components)\f[R]
and double (float64) type values.
It contains the document-wise topic distribution for input data X.
.IP \[bu] 2
\f[B]When X is frovedis-like input:\f[R]
.PD 0
.P
.PD
It returns a FrovedisRowmajorMatrix of shape \f[B](n_samples,
n_components)\f[R] and double (float64) type values, containing
document-wise topic distribution for input data X.
.SS 4. perplexity(X, sub_sampling = False)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A scipy sparse matrix or an instance of
FrovedisCRSMatrix for sparse data of int, float, double (float64) type.
It has shape \f[B](n_samples, n_features)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]sub_sampling\f[B]\f[R]: A boolean parameter that specifies
whether to do sub-sampling or not.
It is simply ignored in frovedis implementation.
(Default: False)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
Calculate approximate perplexity for data X.
.PP
Perplexity is defined as exp(-1.
* log-likelihood per word).
.PP
Ideally, as the number of components increase, the perplexity of model
should decrease.
.PP
For example,
.IP
.nf
\f[C]
perp = lda.perplexity(csr_mat)
print(\[dq]perplexity: %.2f\[dq] % (perp))
\f[R]
.fi
.PP
Ouput
.IP
.nf
\f[C]
perplexity: 8.81
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns float type perplexity score.
.SS 5. score(X, y = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A scipy sparse matrix or an instance of
FrovedisCRSMatrix for sparse data of int, float or double (float64)
type.
It has shape \f[B](n_samples, n_features)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]y\f[B]\f[R]: None or any python array-like object (any
shape).
It is simply ignored in frovedis implementation, just like in
Scikit-learn.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
Calculate approximate log-likelihood as score for data X.
.PP
Likelihood is a measure of how plausible model parameters are, given the
data.
Taking a logarithm makes calculations easier.
All values are negative: when x < 1, log(x) < 0.
.PP
The idea is to search for the largest log-likelihood (good score will be
close to 0).
.PP
For example,
.IP
.nf
\f[C]
print(\[dq]score: %.2f\[dq] % (lda.score(csr_mat)))
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
score: -2.18
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns a float value as likelihood score
.SS 6. get_params(deep = True)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[I]\f[BI]deep\f[I]\f[R]: A boolean parameter, used to get parameters
and their values for an estimator.
If True, it will return the parameters for an estimator and contained
subobjects that are estimators.
(Default: True)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
This method belongs to the BaseEstimator class inherited by
LatentDirichletAllocation.
It is used to get parameters and their values of
LatentDirichletAllocation class.
.PP
For example,
.IP
.nf
\f[C]
print(lda.get_params())  
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
{\[aq]algorithm\[aq]: \[aq]original\[aq], \[aq]batch_size\[aq]: 128, \[aq]doc_topic_prior\[aq]: 0.3333333333333333, 
\[aq]evaluate_every\[aq]: -1, \[aq]explore_iter\[aq]: 0, \[aq]learning_decay\[aq]: 0.7, \[aq]learning_method\[aq]: \[aq]batch\[aq], 
\[aq]learning_offset\[aq]: 10.0, \[aq]max_doc_update_iter\[aq]: 100, \[aq]max_iter\[aq]: 10, 
\[aq]mean_change_tol\[aq]: 0.001, \[aq]n_components\[aq]: 3, \[aq]n_jobs\[aq]: None, \[aq]perp_tol\[aq]: 0.1, 
\[aq]random_state\[aq]: None, \[aq]topic_word_prior\[aq]: 0.3333333333333333, \[aq]total_samples\[aq]: 1000000.0, 
\[aq]verbose\[aq]: 0}
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
A dictionary of parameter names mapped to their values.
.SS 7. set_params(**params)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[I]\f[BI]**params\f[I]\f[R]: All the keyword arguments are passed to
this function as dictionary.
This dictionary contains parameters of an estimator with its given
values to set.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
This method belongs to the BaseEstimator class inherited by
LatentDirichletAllocation, used to set parameter values.
.PP
For example,
.IP
.nf
\f[C]
print(\[dq]get parameters before setting:\[dq]) 
print(lda.get_params())
# User just needs to provide the arguments and internally it will create a 
dictionary over the arguments given by user
lda.set_params(n_components = 4, algorithm = \[aq]dp\[aq]) 
print(\[dq]get parameters after setting:\[dq]) 
print(lda.get_params())
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
get parameters before setting:
{\[aq]algorithm\[aq]: \[aq]original\[aq], \[aq]batch_size\[aq]: 128, \[aq]doc_topic_prior\[aq]: 0.3333333333333333, 
\[aq]evaluate_every\[aq]: -1, \[aq]explore_iter\[aq]: 0, \[aq]learning_decay\[aq]: 0.7, \[aq]learning_method\[aq]: \[aq]batch\[aq], 
\[aq]learning_offset\[aq]: 10.0, \[aq]max_doc_update_iter\[aq]: 100, \[aq]max_iter\[aq]: 10, 
\[aq]mean_change_tol\[aq]: 0.001, \[aq]n_components\[aq]: 3, \[aq]n_jobs\[aq]: None, \[aq]perp_tol\[aq]: 0.1, 
\[aq]random_state\[aq]: None, \[aq]topic_word_prior\[aq]: 0.3333333333333333, \[aq]total_samples\[aq]: 1000000.0, 
\[aq]verbose\[aq]: 0}
get parameters after setting:
{\[aq]algorithm\[aq]: \[aq]dp\[aq], \[aq]batch_size\[aq]: 128, \[aq]doc_topic_prior\[aq]: 0.3333333333333333, 
\[aq]evaluate_every\[aq]: -1, \[aq]explore_iter\[aq]: 0, \[aq]learning_decay\[aq]: 0.7, \[aq]learning_method\[aq]: \[aq]batch\[aq], 
\[aq]learning_offset\[aq]: 10.0, \[aq]max_doc_update_iter\[aq]: 100, \[aq]max_iter\[aq]: 10, 
\[aq]mean_change_tol\[aq]: 0.001, \[aq]n_components\[aq]: 4, \[aq]n_jobs\[aq]: None, \[aq]perp_tol\[aq]: 0.1, 
\[aq]random_state\[aq]: None, \[aq]topic_word_prior\[aq]: 0.3333333333333333, \[aq]total_samples\[aq]: 1000000.0, 
\[aq]verbose\[aq]: 0}
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 8. fit_transform(X, y = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A scipy sparse matrix or an instance of
FrovedisCRSMatrix for sparse data of int, float or double (float64)
type.
It has shape \f[B](n_samples, n_features)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]y\f[B]\f[R]: None or any python array-like object (any
shape).
It is simply ignored in frovedis implementation, like in Scikit-learn.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It performs fit() and transform() on X and y (unused), and returns a
transformed version of X.
.PP
For example,
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
test_corpus = [
\[aq]This is the first second third document.\[aq],
\[aq]This This one third document.\[aq]
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.    
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(test_corpus)

# transform input after fitting   
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3)
print(lda.fit_transform(csr_mat))
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
[[0.04761905 0.61904762 0.47619048]
 [0.06666667 0.66666667 0.46666667]]
\f[R]
.fi
.PP
Like in fit() and transform(), we can also provide frovedis-like input
in fit_transform() for faster computation.
.PP
For example,
.IP
.nf
\f[C]
# loading sample csr matrix data representing the input corpus-vocabulary count 
test_corpus = [
\[aq]This is the first second third document.\[aq],
\[aq]This This one third document.\[aq]
]

# feature transformers CountVectorizer can be used for converting text to word count vectors.
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
csr_mat = vectorizer.fit_transform(test_corpus)

# Since \[dq]csr_mat\[dq] is scipy sparse data, we have created FrovedisCRSMatrix
# Also it only supports int64 as itype for input sparse data during training
from frovedis.matrix.crs import FrovedisCRSMatrix
cmat = FrovedisCRSMatrix(csr_mat, dtype = np.int32, itype = np.int64)

# transform input according to trained model 
from frovedis.mllib.decomposition import LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components = 3)
lda.fit_transform(cmat).debug_print()
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
matrix:
num_row = 2, num_col = 3
node 0
node = 0, local_num_row = 2, local_num_col = 3, val = 0.333333 0.619048 0.190476 
0.266667 0.666667 0.266667 
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
- \f[B]When X is python native input:\f[R]
.PD 0
.P
.PD
It returns a numpy ndarray of shape \f[B](n_samples, n_components)\f[R]
and double (float64) type values.
This is a transformed array X.
.IP \[bu] 2
\f[B]When X is frovedis-like input:\f[R]
.PD 0
.P
.PD
It returns a FrovedisRowmajorMatrix of shape \f[B](n_samples,
n_components)\f[R] and double (float64) type values.
This is a transformed array X.
.SS 9. load(fname, dtype = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fname\f[B]\f[R]: A string object containing the name of the
file having model information to be loaded.
.PD 0
.P
.PD
\f[B]\f[BI]dtype\f[B]\f[R]: A data-type is inferred from the input data.
Currently, expected input data-type is either float or double (float64).
(Default: None)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It loads the model from the specified file (having little-endian binary
data).
.PP
For example,
.IP
.nf
\f[C]
lda.load(\[dq]./out/trained_lda_model\[dq])
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 10. save(fname)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fname\f[B]\f[R]: A string object containing the name of the
file on which the target model is to be saved.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
On success, it writes the model information (corpus_topic, metadata and
word_topic) in the specified file as little-endian binary data.
Otherwise, it throws an exception.
.PP
For example,
.IP
.nf
\f[C]
# To save the lda model
lda.save(\[dq]./out/trained_lda_model\[dq])  
\f[R]
.fi
.PP
This will save the lda model on the path
\[lq]/out/trained_lda_model\[rq].
.PD 0
.P
.PD
It would raise exception if the directory already exists with same name.
.PP
The `trained_lda_model' directory has
.PP
\f[B]trained_lda_model\f[R]
.PD 0
.P
.PD
|\[em]\[en]\f[B]corpus_topic\f[R]
.PD 0
.P
.PD
|\[em]\[en]metadata
.PD 0
.P
.PD
|\[em]\[en]\f[B]word_topic\f[R]
.PP
Here, the \f[B]corpus_topic\f[R] directory contains the corpus topic
count as sparse matrix in binary format.
.PD 0
.P
.PD
The metadata file contains the information of model kind, input datatype
used for trained model.
.PD 0
.P
.PD
Here, the \f[B]word_topic\f[R] directory contains the word topic count
as sparse matrix in binary format.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing.
.SS 11. release()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It can be used to release the in-memory model at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
lda.release()
\f[R]
.fi
.PP
This will reset the after-fit populated attributes (like components_) to
None, along with releasing server side memory.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing.
.SS 12. is_fitted()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It can be used to confirm if the model is already fitted or not.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns `True', if the model is already fitted, otherwise, it returns
`False'.
.SH SEE ALSO
.IP \[bu] 2
\f[B]Introduction to FrovedisCRSMatrix\f[R]
