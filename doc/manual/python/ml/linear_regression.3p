.\" Automatically generated by Pandoc 2.17.1.1
.\"
.\" Define V font for inline verbatim, using C font in formats
.\" that render this, and otherwise B font.
.ie "\f[CB]x\f[]"x" \{\
. ftr V B
. ftr VI BI
. ftr VB B
. ftr VBI BI
.\}
.el \{\
. ftr V CR
. ftr VI CI
. ftr VB CB
. ftr VBI CBI
.\}
.TH "Linear Regression" "" "" "" ""
.hy
.SH NAME
.PP
Linear Regression - A regression algorithm used to predict the
continuous output without any regularization.
.SH SYNOPSIS
.IP
.nf
\f[C]
class frovedis.mllib.linear_model.LinearRegression(fit_intercept=True, normalize=False,  
                                                   copy_X=True, n_jobs=None,  
                                                   max_iter=None, tol=0.0001,  
                                                   lr_rate=1e-8, solver=None,  
                                                   verbose=0, warm_start=False)  
\f[R]
.fi
.SS Public Member Functions
.PP
fit(X, y, sample_weight = None)
.PD 0
.P
.PD
predict(X)
.PD 0
.P
.PD
score(X, y, sample_weight = None)
.PD 0
.P
.PD
get_params(deep = True)
.PD 0
.P
.PD
set_params(**params)
.PD 0
.P
.PD
load(fname, dtype = None)
.PD 0
.P
.PD
save(fname)
.PD 0
.P
.PD
debug_print()
.PD 0
.P
.PD
release()
.PD 0
.P
.PD
is_fitted()
.SH DESCRIPTION
.PP
Linear least squares is the most common formulation for regression
problems.
It is a linear method with the loss function given by the \f[B]squared
loss\f[R]:
.IP
.nf
\f[C]
L(w;x,y) := 1/2(wTx-y)\[ha]2
\f[R]
.fi
.PP
During training, the input \f[B]X\f[R] is the training data and
\f[B]y\f[R] is the corresponding label values which we want to predict.
\f[B]w\f[R] is the linear model (also known as weight) which uses a
single weighted sum of features to make a prediction.
The method is called linear since it can be expressed as a function of
wTx and y.
\f[B]Linear Regression does not use any regularizer.\f[R]
.PP
The gradient of the squared loss is: \f[B](wTx-y).x\f[R]
.PP
Frovedis provides implementation of linear regression with the following
optimizers:
.PD 0
.P
.PD
(1) stochastic gradient descent with minibatch
.PD 0
.P
.PD
(2) LBFGS optimizer
.PD 0
.P
.PD
(3) least-square
.PP
The simplest method to solve optimization problems of the form \f[B]min
f(w)\f[R] is gradient descent.
Such first-order optimization methods well-suited for large-scale and
distributed computation.
Whereas, L-BFGS is an optimization algorithm in the family of
quasi-Newton methods to solve the optimization problems of the similar
form.
.PP
Like the original BFGS, L-BFGS (Limited Memory BFGS) uses an estimation
to the inverse Hessian matrix to steer its search through feature space,
but where BFGS stores a dense nxn approximation to the inverse Hessian
(n being the number of features in the problem), L-BFGS stores only a
few vectors that represent the approximation implicitly.
L-BFGS often achieves rapider convergence compared with other
first-order optimization.
.PP
For least-square solver, we have used LAPACK routine \[lq]gelsd\[rq] and
ScaLAPACK routine \[lq]gels\[rq] when input data is dense in nature.
For the sparse-input we have provided a least-square implementation,
similar to \f[B]scipy.sparse.linalg.lsqr\f[R].
.PP
This module provides a client-server implementation, where the client
application is a normal python program.
The frovedis interface is almost same as Scikit-learn Linear Regression
interface, but it doesn\[cq]t have any dependency with Scikit-learn.
It can be used simply even if the system doesn\[cq]t have Scikit-learn
installed.
Thus in this implementation, a python client can interact with a
frovedis server by sending the required python data for training at
frovedis side.
Python data is converted into frovedis compatible data internally and
the python ML call is linked with the respective frovedis ML call to get
the job done at frovedis server.
.PP
Python side calls for Linear Regression on the frovedis server.
Once the training is completed with the input data at the frovedis
server, it returns an abstract model with a unique model ID to the
client python program.
.PP
When prediction-like request would be made on the trained model, python
program will send the same request to the frovedis server.
After the request is served at the frovedis server, the output would be
sent back to the python client.
.SS Detailed Description
.SS 1. LinearRegression()
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fit_intercept\f[B]\f[R]: A boolean parameter specifying
whether a constant(intercept) should be added to the decision function.
(Default: True)
.PD 0
.P
.PD
\f[B]\f[BI]normalize\f[B]\f[R]: An unused parameter.
(Default: False)
.PD 0
.P
.PD
\f[B]\f[BI]copy_X\f[B]\f[R]: An unused parameter.
(Default: True)
.PD 0
.P
.PD
\f[B]\f[BI]n_jobs\f[B]\f[R]: An unused parameter.
(Default: None)
.PD 0
.P
.PD
\f[B]\f[BI]max_iter\f[B]\f[R]: A positive integer parameter used to set
the maximum number of iterations.
When it is None(not specified explicitly), it will be set as 1000 for
\[lq]sag\[rq], \[lq]lbfgs\[rq], \[lq]lapack\[rq] and \[lq]scalapack\[rq]
solvers and for \[lq]sparse_lsqr\[rq] solver, it will be 2 *
(n_features).
(Default: None)
.PD 0
.P
.PD
\f[B]\f[BI]tol\f[B]\f[R]: Zero or a positive value of double (float64)
type specifying the convergence tolerance value.
(Default: 0.001)
.PD 0
.P
.PD
\f[B]\f[BI]lr_rate\f[B]\f[R]: A positive value of double (float64) type
containing the learning rate.
(Default: 1e-8)
.PD 0
.P
.PD
\f[B]\f[BI]solver\f[B]\f[R]: A string parameter specifying the solver to
use.
(Default: None).
.PD 0
.P
.PD
When it is None (not explicitly specified), the value will be set to
\[lq]lapack\[rq] when dense input matrix (X) is provided and for sparse
input matrix (X), it is set as \[lq]sparse_lsqr\[rq].
Frovedis supports \[lq]sag\[rq], \[lq]lbfgs\[rq], \[lq]lapack\[rq],
\[lq]scalapack\[rq], \[lq]sparse_lsqr\[rq] solvers.
.PD 0
.P
.PD
\[lq]lapack\[rq] and \[lq]scalapack\[rq] solvers can only work with
dense data.
.PD 0
.P
.PD
\[lq]sparse_lsqr\[rq] solver can only work with sparse data.
.PD 0
.P
.PD
\f[B]\f[BI]verbose\f[B]\f[R]: An integer parameter specifying the log
level to use.
Its value is 0 by default (for INFO mode and not specified explicitly).
But it can be set to 1 (for DEBUG mode) or 2 (for TRACE mode) for
getting training time logs from frovedis server.
.PD 0
.P
.PD
\f[B]\f[BI]warm_start\f[B]\f[R]: A boolean parameter which when set to
True, reuses the solution of the previous call to fit as initialization,
otherwise, just erase the previous solution.
Only supported by \[lq]sag\[rq] and \[lq]lbfgs\[rq] solvers.
(Default: False)
.PP
\f[B]Attributes\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]coef_\f[B]\f[R]: It is a python ndarray(containing float or
double (float64) typed values depending on data-type of input matrix
(X)) of estimated coefficients for the linear regression problem.
It has shape \f[B](n_features,)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]rank_\f[B]\f[R]: An integer value used to store rank of
matrix (X).
It is only available when matrix (X) is dense and \[lq]lapack\[rq]
solver is used.
.PD 0
.P
.PD
\f[B]\f[BI]singular_\f[B]\f[R]: It is a python ndarray(contaning float
or double (float64) typed values depending on data-type of input matrix
(X)) and has shape \f[B](min(X,y),)\f[R] which is used to store singular
values of X.
It is only available when X is dense and \[lq]lapack\[rq] solver is
used.
.PD 0
.P
.PD
\f[B]\f[BI]intercept_(bias)\f[B]\f[R]: It is a python ndarray(contaning
float or double (float64) typed values depending on data-type of input
matrix (X)).
If fit_intercept is set to False, the intercept is set to zero.
It has shape \f[B](1,)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]n_iter_\f[B]\f[R]: A positive integer value used to get the
actual iteration point at which the problem is converged.
It is only available for \[lq]sag\[rq], \[lq]lbfgs\[rq] and
\[lq]sparse-lsqr\[rq] solvers.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It initializes a Linear Regression object with the given parameters.
.PP
The parameters: \[lq]normalize\[rq], \[lq]copy_X\[rq] and
\[lq]n_jobs\[rq] are simply kept in to to make the interface uniform to
the Scikit-learn Linear Regression module.
They are not used anywhere within the frovedis implementation.
.PP
\[lq]solver\[rq] can be \[lq]sag\[rq] for frovedis side stochastic
gradient descent, \[lq]lbfgs\[rq] for frovedis side LBFGS optimizer,
\[lq]sparse_lsqr\[rq], \[lq]lapack\[rq] and \[lq]scalapack\[rq] when
optimizing the linear regression model.
.PP
\[lq]max_iter\[rq] can be used to set the maximum interations to achieve
the convergence.
In case the convergence is not achieved, it displays a warning for the
same.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 2. fit(X, y, sample_weight = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A numpy dense or scipy sparse matrix or any
python array-like object or an instance of FrovedisCRSMatrix for sparse
data and FrovedisColmajorMatrix for dense data.
.PD 0
.P
.PD
\f[B]\f[BI]y\f[B]\f[R]: Any python array-like object or an instance of
FrovedisDvector containing the target values.
It has shape \f[B](n_samples,)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]sample_weight\f[B]\f[R]: Python ndarray containing the
intended weights for each input samples and it should be the shape of
\f[B](n_samples,)\f[R].
.PD 0
.P
.PD
When it is None (not specified explicitly), an uniform weight vector is
assigned on each input sample.
(Default: None)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It accepts the training feature matrix (X) and corresponding output
labels (y) as inputs from the user and trains a linear regression model
with those data at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
# loading a sample matrix and labels data
from sklearn.datasets import load_boston
mat, label = load_boston(return_X_y = True)

# fitting input matrix and label on linear regression object
from frovedis.mllib.linear_model import LinearRegression
lr = LinearRegression(solver = \[aq]sag\[aq]).fit(mat,label)
\f[R]
.fi
.PP
When native python data is provided, it is converted to frovedis-like
inputs and sent to frovedis server which consumes some data transfer
time.
Pre-constructed frovedis-like inputs can be used to speed up the
training time, especially when same data would be used for multiple
executions.
.PP
For example,
.IP
.nf
\f[C]
# loading a sample matrix and labels data
from sklearn.datasets import load_boston
mat, label = load_boston(return_X_y = True)

# Since \[dq]mat\[dq] is numpy dense data, we have created FrovedisColmajorMatrix. 
# For scipy sparse data, FrovedisCRSMatrix should be used instead.
from frovedis.matrix.dense import FrovedisColmajorMatrix
from frovedis.matrix.dvector import FrovedisDvector 
cmat = FrovedisColmajorMatrix(mat)
dlbl = FrovedisDvector(lbl)

# Linear Regression with pre-constructed frovedis-like inputs
from frovedis.mllib.linear_model import LinearRegression
lr = LinearRegression(solver = \[aq]sag\[aq]).fit(cmat, dlbl)  
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 3. predict(X)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A numpy dense or scipy sparse matrix or any
python array-like object or an instance of FrovedisCRSMatrix for sparse
data and FrovedisRowmajorMatrix for dense data.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It accepts the test feature matrix (X) in order to make prediction on
the trained model at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
# predicting on sag linear regression model
lr.predict(mat[:10])  
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
[30.00384338 25.02556238 30.56759672 28.60703649 27.94352423 25.25628446
 23.00180827 19.53598843 11.52363685 18.92026211]
\f[R]
.fi
.PP
Like in fit(), frovedis-like input can be used to speed-up the
prediction making on the trained model at server side.
.PP
For example,
.IP
.nf
\f[C]
# Since \[dq]cmat\[dq] is FrovedisColmajorMatrix, we have created FrovedisRowmajorMatrix.
from frovedis.matrix.dense import FrovedisRowmajorMatrix

# predicting on sag linear regression model using pre-constructed input
lr.predict(cmat.to_frovedis_rowmatrix())
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
[30.00384338 25.02556238 30.56759672 28.60703649 27.94352423 25.25628446 
 23.00180827 19.53598843 11.52363685 18.92026211]
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns a numpy array of float or double (float64) type and has shape
\f[B](n_samples,)\f[R] containing the predicted outputs.
.SS 4. score(X, y, sample_weight = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]X\f[B]\f[R]: A numpy dense or scipy sparse matrix or any
python array-like object or an instance of FrovedisCRSMatrix for sparse
data and FrovedisRowmajorMatrix for dense data.
.PD 0
.P
.PD
\f[B]\f[BI]y\f[B]\f[R]: Any python array-like object containing the true
values for X.
It has shape \f[B](n_samples,)\f[R].
.PD 0
.P
.PD
\f[B]\f[BI]sample_weight\f[B]\f[R]: Python ndarray containing the
intended weights for each input samples and it should be the shape of
\f[B](n_samples,)\f[R].
.PD 0
.P
.PD
When it is None (not specified explicitly), an uniform weight vector is
assigned on each input sample.
(Default: None)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
Calculate the root mean square value on the given test data and labels
i.e.\  R2(r-squared) of self.predict(X) wrt.
y.
.PP
The coefficient `R2' is defined as (1 - (u/v)),
.PD 0
.P
.PD
where `u' is the residual sum of squares ((y_true - y_pred) ** 2).sum()
and,
.PD 0
.P
.PD
`v' is the total sum of squares ((y_true - y_true.mean()) ** 2).sum().
.PD 0
.P
.PD
The best possible score is 1.0 and it can be negative (because the model
can be arbitrarily worse).
A constant model that always predicts the expected value of y,
disregarding the input features, would get a R2 score of 0.0.
.PP
For example,
.IP
.nf
\f[C]
# calculate R2 score on given test data and labels
lr.score(mat[:10], label[:10]) 
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
0.40
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns an R2 score of float type.
.SS 5. get_params(deep = True)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[I]\f[BI]deep\f[I]\f[R]: A boolean parameter, used to get parameters
and their values for an estimator.
If True, it will return the parameters for an estimator and contained
subobjects that are estimators.
(Default: True)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
This method belongs to the BaseEstimator class inherited by
LinearRegression.
It is used to get parameters and their values of LinearRegression class.
.PP
For example,
.IP
.nf
\f[C]
print(lr.get_params())
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
{\[aq]copy_X\[aq]: True, \[aq]fit_intercept\[aq]: True, \[aq]lr_rate\[aq]: 1e-08, \[aq]max_iter\[aq]: None, \[aq]n_jobs\[aq]: None,
\[aq]normalize\[aq]: False, \[aq]solver\[aq]: \[aq]lapack\[aq], \[aq]tol\[aq]: 0.0001, \[aq]verbose\[aq]: 0, \[aq]warm_start\[aq]: False}  
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
A dictionary of parameter names mapped to their values.
.SS 6. set_params(**params)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[I]\f[BI]**params\f[I]\f[R]: All the keyword arguments are passed to
this function as dictionary.
This dictionary contains parameters of an estimator with its given
values to set.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
This method belongs to the BaseEstimator class inherited by
LinearRegression, used to set parameter values.
.PP
For example,
.IP
.nf
\f[C]
print(\[dq]get parameters before setting:\[dq])  
print(lr.get_params())  
# User just needs to provide the arguments and internally it will create a 
dictionary over the arguments given by user  
lr.set_params(solver=\[aq]lbfgs\[aq], max_iter = 10000)  
print(\[dq]get parameters after setting:\[dq])  
print(lr.get_params())  
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
get parameters before setting:
{\[aq]copy_X\[aq]: True, \[aq]fit_intercept\[aq]: True, \[aq]lr_rate\[aq]: 1e-08, \[aq]max_iter\[aq]: None, \[aq]n_jobs\[aq]: None,
\[aq]normalize\[aq]: False, \[aq]solver\[aq]: \[aq]lapack\[aq], \[aq]tol\[aq]: 0.0001, \[aq]verbose\[aq]: 0, \[aq]warm_start\[aq]: False}
get parameters after setting:
{\[aq]copy_X\[aq]: True, \[aq]fit_intercept\[aq]: True, \[aq]lr_rate\[aq]: 1e-08, \[aq]max_iter\[aq]: 10000, \[aq]n_jobs\[aq]: None,
\[aq]normalize\[aq]: False, \[aq]solver\[aq]: \[aq]lbfgs\[aq], \[aq]tol\[aq]: 0.0001, \[aq]verbose\[aq]: 0, \[aq]warm_start\[aq]: False}
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 7. load(fname, dtype = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fname\f[B]\f[R]: A string object containing the name of the
file having model information to be loaded.
.PD 0
.P
.PD
\f[B]\f[BI]dtype\f[B]\f[R]: A data-type is inferred from the input data.
Currently, expected input data-type is either float or double (float64).
(Default: None)
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It loads the model from the specified file (having little-endian binary
data).
.PP
For example,
.IP
.nf
\f[C]
lr.load(\[dq]./out/LNRModel\[dq])
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 8. save(fname)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fname\f[B]\f[R]: A string object containing the name of the
file on which the target model is to be saved.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
On success, it writes the model information (metadata and model) in the
specified file as little-endian binary data.
Otherwise, it throws an exception.
.PP
For example,
.IP
.nf
\f[C]
# To save the linear regression model
lr.save(\[dq]./out/LNRModel\[dq])    
\f[R]
.fi
.PP
This will save the linear regression model on the path
\[lq]/out/LNRModel\[rq].
It would raise exception if the directory already exists with same name.
.PP
The `LNRModel' directory has
.PP
\f[B]LNRModel\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] metadata
.PD 0
.P
.PD
|\[em]\[em]\[en] model
.PP
The metadata file contains the number of classes, model kind, input
datatype used for trained model.
.PD 0
.P
.PD
Here, the model file contains information about weights, intercept.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing.
.SS 9. debug_print()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It shows the target model information (weight values, intercept) on the
server side user terminal.
It is mainly used for debugging purpose.
.PP
For example,
.IP
.nf
\f[C]
lr.debug_print() 
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
-------- Weight Vector:: --------
-0.108011 0.0464205 0.0205586 2.68673 -17.7666 3.80987 0.000692225 -1.47557 0.306049 
-0.0123346 -0.952747 0.00931168 -0.524758
Intercept:: 36.4595
\f[R]
.fi
.PP
This output will be visible on server side.
It displays the weights and intercept values on the trained model which
is currently present on the server.
.PP
\f[B]No such output will be visible on client side.\f[R]
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing.
.SS 10. release()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It can be used to release the in-memory model at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
lr.release()
\f[R]
.fi
.PP
This will reset the after-fit populated attributes to None, along with
releasing server side memory.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing.
.SS 11. is_fitted()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It can be used to confirm if the model is already fitted or not.
In case, predict() is used before training the model, then it can prompt
the user to train the linear regression model first.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns `True', if the model is already fitted otherwise, it returns
`False'.
.SH SEE ALSO
.IP \[bu] 2
\f[B]Introduction to FrovedisRowmajorMatrix\f[R]
.PD 0
.P
.PD
.IP \[bu] 2
\f[B]Introduction to FrovedisColmajorMatrix\f[R]
.PD 0
.P
.PD
.IP \[bu] 2
\f[B]Introduction to FrovedisCRSMatrix\f[R]
.PD 0
.P
.PD
.IP \[bu] 2
\f[B]Introduction to FrovedisDvector\f[R]
.PD 0
.P
.PD
.IP \[bu] 2
\f[B]Lasso Regression in Frovedis\f[R]
.PD 0
.P
.PD
.IP \[bu] 2
\f[B]Ridge Regression in Frovedis\f[R]
