.TH "Logistic Regression" "" "" "" ""
.SH NAME
.PP
Logistic Regression \- A classification algorithm to predict the binary
and multi\-class output with logistic loss.
.SH SYNOPSIS
.PP
class
frovedis.mllib.linear_model.LogisticRegression(penalty=\[aq]l2\[aq],
dual=False, tol=1e\-4,
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ C=100.0,
fit_intercept=True,
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ intercept_scaling=1,
class_weight=None,
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ random_state=None,
solver=\[aq]lbfgs\[aq],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ max_iter=1000,
multi_class=\[aq]auto\[aq],
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ verbose=0,
warm_start=False,
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ n_jobs=1,
l1_ratio=None,
.PD 0
.P
.PD
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ lr_rate=0.01,
use_shrink=False)
.SS Public Member Functions
.PP
fit(X, y, sample_weight = None)
.PD 0
.P
.PD
predict(X)
.PD 0
.P
.PD
predict_proba(X)
.PD 0
.P
.PD
score(X, y, sample_weight = None)
.PD 0
.P
.PD
load(fname, dtype = None)
.PD 0
.P
.PD
save(fname)
.PD 0
.P
.PD
debug_print()
.PD 0
.P
.PD
release()
.PD 0
.P
.PD
is_fitted()
.SH DESCRIPTION
.PP
Classification aims to divide the items into categories.
The most common classification type is binary classification, where
there are two categories, usually named positive and negative.
The other is multinomial classification, where there are more than two
categories.
\f[B]Frovedis supports both binary and multinomial logistic regression
algorithms.\f[] For multinomial classification, it uses softmax
probability.
.PP
Logistic regression is widely used to predict a binary response.
It is a linear method with the loss function given by the \f[B]logistic
loss\f[]:
.IP
.nf
\f[C]
L(w;x,y)\ :=\ log(1\ +\ exp(\-ywTx))\ \ \ \ 
\f[]
.fi
.PP
Where the vectors x are the training data examples and y are their
corresponding labels (Frovedis supports any values as for labels, but
internally it encodes the input binary labels to \-1 and 1, and input
multinomial labels to 0, 1, 2, ..., N\-1 (where N is the no.
of classes) before training at Frovedis server) which we want to
predict.
w is the linear model (also called as weight) which uses a single
weighted sum of features to make a prediction.
Frovedis Logistic Regression supports ZERO, L1 and L2 regularization to
address the overfit problem.
However, LBFGS solver supports only L2 regularization.
.PD 0
.P
.PD
The gradient of the logistic loss is: \-y( 1 \- 1 / (1 + exp(\-ywTx))).x
.PD 0
.P
.PD
The gradient of the L1 regularizer is: sign(w)
.PD 0
.P
.PD
And, the gradient of the L2 regularizer is: w
.PD 0
.P
.PD
For binary classification problems, the algorithm outputs a binary
logistic regression model.
Given a new data point, denoted by x, the model makes predictions by
applying the logistic function:
.IP
.nf
\f[C]
f(z)\ :=\ 1\ /\ 1\ +\ exp(\-z)\ \ 
\f[]
.fi
.PP
Where z = wTx.
By default (threshold=0.5), if f(wTx) > 0.5, the response is positive
(1), else the response is negative (0).
.PP
In the multiclass case, the training algorithm uses the one\-vs\-rest
(OvR) scheme.
Currently the "multinomial" option is supported only by the "sag"
solvers.
.PD 0
.P
.PD
Frovedis provides implementation of logistic regression with two
different optimizers:
.PD 0
.P
.PD
(1) stochastic gradient descent with minibatch
.PD 0
.P
.PD
(2) LBFGS optimizer
.PD 0
.P
.PD
They can handle both dense and sparse input.
.PP
The simplest method to solve optimization problems of the form \f[B]min
f(w)\f[] is gradient descent.
Such first\-order optimization methods well\-suited for large\-scale and
distributed computation.
Whereas, L\-BFGS is an optimization algorithm in the family of
quasi\-Newton methods to solve the optimization problems of the similar
form.
.PP
Like the original BFGS, L\-BFGS (Limited Memory BFGS) uses an estimation
to the inverse Hessian matrix to steer its search through feature space,
but where BFGS stores a dense nxn approximation to the inverse Hessian
(n being the number of features in the problem), L\-BFGS stores only a
few vectors that represent the approximation implicitly.
L\-BFGS often achieves rapider convergence compared with other
first\-order optimization.
.PP
This module provides a client\-server implementation, where the client
application is a normal python program.
The frovedis interface is almost same as Scikit\-learn Logistic
Regression interface, but it doesn\[aq]t have any dependency with
Scikit\-learn.
It can be used simply even if the system doesn\[aq]t have Scikit\-learn
installed.
Thus in this implementation, a python client can interact with a
frovedis server sending the required python data for training at
frovedis side.
Python data is converted into frovedis compatible data internally and
the python ML call is linked with the respective frovedis ML call to get
the job done at frovedis server.
.PP
Python side calls for Logistic Regression on the frovedis server.
Once the training is completed with the input data at the frovedis
server, it returns an abstract model with a unique model ID to the
client python program.
.PP
When prediction\-like request would be made on the trained model, python
program will send the same request to the frovedis server.
After the request is served at the frovedis server, the output would be
sent back to the python client.
.SS Detailed Description
.SS 1. LogisticRegression()
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]penalty\f[]\f[]: A string object containing the regularizer
type to use.
Currenlty none, l1 and l2 are supported by Frovedis.
(Default: \[aq]l2\[aq])
.PD 0
.P
.PD
\f[B]\f[I]dual\f[]\f[]: A boolean parameter.
(unused)
.PD 0
.P
.PD
\f[B]\f[I]tol\f[]\f[]: A double(float64) type value specifying the
convergence tolerance value.
It must be zero or a postive value.
(Default: 1e\-4)
.PD 0
.P
.PD
\f[B]\f[I]C\f[]\f[]: A positive float parameter, it is the inverse of
regularization strength.
Like in support vector machines, smaller values specify stronger
regularization.
(Default: 100.0)
.PD 0
.P
.PD
\f[B]\f[I]fit_intercept\f[]\f[]: A boolean parameter specifying whether
a constant (intercept) should be added to the decision function.
(Default: True)
.PD 0
.P
.PD
\f[B]\f[I]intercept_scaling\f[]\f[]: An integer parameter.
(unused)
.PD 0
.P
.PD
\f[B]\f[I]class_weight\f[]\f[]: A python dictionary or a string object.
(unused)
.PD 0
.P
.PD
\f[B]\f[I]random_state\f[]\f[]: An integer, None or RandomState
instance.
(unused)
.PD 0
.P
.PD
\f[B]\f[I]solver\f[]\f[]: A string object specifying the solver to use.
(Default: \[aq]lbfgs\[aq])
.PD 0
.P
.PD
It can be "sag" for frovedis side stochastic gradient descent or "lbfgs"
for frovedis side LBFGS optimizer when optimizing the logistic
regression model.
.PD 0
.P
.PD
"sag" can handle L1, L2 or no penalty.
.PD 0
.P
.PD
"lbfgs" can handle only L2 penalty.
.PD 0
.P
.PD
\f[B]\f[I]max_iter\f[]\f[]: A positive integer value specifying maximum
iteration count.
(Default: 1000)
.PD 0
.P
.PD
\f[B]\f[I]multi_class\f[]\f[]: A string object specifying the type of
classification.
If it is "auto" or "ovr", then a binary classification is selected when
N = 2, otherwise multinomial classification is selected (where N is the
no.
of classes in training labels).
If it is "multinomial", then it always selects a multinomial problem
(even when N = 2).
Only "sag" solvers support multinomial classification currently.
(Default: \[aq]auto\[aq])
.PD 0
.P
.PD
\f[B]\f[I]verbose\f[]\f[]: An integer parameter specifying the log level
to use.
Its value is 0 by default(for INFO mode and not explicitly specified).
But it can be set to 1 (for DEBUG mode) or 2 (for TRACE mode) for
getting training time logs from frovedis server.
.PD 0
.P
.PD
\f[B]\f[I]warm_start\f[]\f[]: A boolean parameter which when set to
True, reuses the solution of the previous call to fit as initialization,
otherwise, just erase the previous solution.
(Default: False)
.PD 0
.P
.PD
\f[B]\f[I]n_jobs\f[]\f[]: An integer parameter.
(unused)
.PD 0
.P
.PD
\f[B]\f[I]l1_ratio\f[]\f[]: A float parameter, also called the
Elastic\-Net mixing parameter.
(unused)
.PD 0
.P
.PD
\f[B]\f[I]lr_rate(alpha)\f[]\f[]: A double(float64) parameter containing
the learning rate.
(Default: 0.01)
.PD 0
.P
.PD
\f[B]\f[I]use_shrink\f[]\f[]: A boolean parameter applicable only for
"sag" solver with "sparse" input (X).
When set to True for sparse input, it can improve training performance
by reducing communication overhead across participating processes.
(Default: False)
.PP
\f[B]Attributes\f[]
.PD 0
.P
.PD
\f[B]\f[I]coef_\f[]\f[]: It is a python ndarray(float or double(float64)
values depending on input matrix data type) of coefficient of the
features in the decision function.
It has shape (1, n_features) when the given problem is "binary" and
(n_classes, n_features) when it is a "multinomial" problem.
.PD 0
.P
.PD
\f[B]\f[I]intercept_(bias)\f[]\f[]: It is a python ndarray(float or
double(float64) values depending on input matrix data type) If
fit_intercept is set to False, the intercept is set to zero.
It has shape (1,) when the given problem is "binary" and (n_classes)
when its "multinomial" problem.
.PD 0
.P
.PD
\f[B]\f[I]classes_\f[]\f[]: It is a python ndarray(any type) of unique
labels given to the classifier during training.
It has shape (n_classes,).
.PD 0
.P
.PD
\f[B]\f[I]n_iter_\f[]\f[]: It is a python ndarray of shape(1,) and has
integer data.
It is used to get the actual iteration point at which the problem is
converged.
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It initializes a Logistic Regression object with the given parameters.
.PP
The parameters: "dual", "intercept_scaling", "class_weight",
"random_state", "n_jobs" and "l1_ratio" are simply kept to make the
interface uniform to the Scikit\-learn Logistic Regression module.
They are not used anywhere within the frovedis implementation.
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It simply returns "self" reference.
.SS 2. fit(X, y, sample_weight = None)
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]X\f[]\f[]: A numpy dense or scipy sparse matrix or any python
array\-like object or an instance of FrovedisCRSMatrix for sparse data
and FrovedisColmajorMatrix for dense data.
.PD 0
.P
.PD
\f[B]\f[I]y\f[]\f[]: Any python array\-like object or an instance of
FrovedisDvector.
.PD 0
.P
.PD
\f[B]\f[I]sample_weight\f[]\f[]: A python narray containing the intended
weights for each input samples and it should be the shape of (nsamples,
).
When it is None (not specified explicitly), an uniform weight vector is
assigned on each input sample.
(Default: None)
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It accepts the training feature matrix (X) and corresponding output
labels (y) as inputs from the user and trains a logistic regression
model with specifed regularization with those data at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
#\ loading\ a\ sample\ matrix\ and\ labels\ data
from\ sklearn.datasets\ import\ load_breast_cancer
mat,\ lbl\ =\ load_breast_cancer(return_X_y\ =\ True)

#\ fitting\ input\ matrix\ and\ label\ on\ LogisticRegression\ object
from\ frovedis.mllib.linear_model\ import\ LogisticRegression
lr\ =\ LogisticRegression(solver\ =\ \[aq]lbfgs\[aq]).fit(mat,lbl)
\f[]
.fi
.PP
When native python data is provided, it is converted to frovedis\-like
inputs and sent to frovedis server which consumes some data transfer
time.
Pre\-constructed frovedis\-like inputs can be used to speed up the
training time, specially when same data would be used for multiple
executions.
.PP
For example,
.IP
.nf
\f[C]
#\ loading\ a\ sample\ matrix\ and\ labels\ data
from\ sklearn.datasets\ import\ load_breast_cancer
mat,\ lbl\ =\ load_breast_cancer(return_X_y\ =\ True)

#\ Since\ "mat"\ is\ numpy\ dense\ data,\ we\ have\ created\ FrovedisColmajorMatrix.\ 
#\ For\ scipy\ sparse\ data,\ FrovedisCRSMatrix\ should\ be\ used\ instead.
from\ frovedis.matrix.dense\ import\ FrovedisColmajorMatrix
from\ frovedis.matrix.dvector\ import\ FrovedisDvector\ 
cmat\ =\ FrovedisColmajorMatrix(mat)
dlbl\ =\ FrovedisDvector(lbl)

#\ Logistic\ Regression\ with\ pre\-constructed\ frovedlis\-like\ inputs
from\ frovedis.mllib.linear_model\ import\ LogisticRegression
lr\ =\ LogisticRegression(solver\ =\ \[aq]lbfgs\[aq]).fit(cmat,\ dlbl)
\f[]
.fi
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It simply returns "self" reference.
.SS 3. predict(X)
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]X\f[]\f[]: A numpy dense or scipy sparse matrix or any python
array\-like object or an instance of FrovedisCRSMatrix for sparse data
and FrovedisRowmajorMatrix for dense data.
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It accepts the test feature matrix (X) in order to make prediction on
the trained model at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
#\ predicting\ on\ lbfgs\ logistic\ regression\ model
lr.predict(mat)\ \ 
\f[]
.fi
.PP
Output
.IP
.nf
\f[C]
[0\ 0\ 0\ ...\ 0\ 0\ 1]
\f[]
.fi
.PP
Like in fit(), frovedis\-like input can be used to speed\-up the
prediction making on the trained model at server side.
.PP
For example,
.IP
.nf
\f[C]
#\ Since\ "cmat"\ is\ FrovedisColmajorMatrix,\ we\ have\ created\ FrovedisRowmajorMatrix.
from\ frovedis.matrix.dense\ import\ FrovedisRowmajorMatrix

#\ predicting\ on\ lbfgs\ logistic\ regression\ model\ using\ pre\-constructed\ input
lr.predict(cmat.to_frovedis_rowmatrix())
\f[]
.fi
.PP
Output
.IP
.nf
\f[C]
[0\ 0\ 0\ ...\ 0\ 0\ 1]
\f[]
.fi
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns a numpy array of float or double(float64) type and of shape
(n_samples,) containing the predicted outputs.
.SS 4. predict_proba(X)
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]X\f[]\f[]: A numpy dense or scipy sparse matrix or any python
array\-like object or an instance of FrovedisCRSMatrix for sparse data
and FrovedisRowmajorMatrix for dense data.
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It accepts the test feature matrix (X) in order to make prediction on
the trained model at frovedis server.
But unlike predict(), it returns the softmax probability matrix of shape
(n_samples, n_classes) containing the probability of each class in each
sample.
.PP
For example,
.IP
.nf
\f[C]
#\ finds\ the\ probablity\ sample\ for\ each\ class\ in\ the\ model
lr.predict_proba(mat)\ \ 
\f[]
.fi
.PP
Output
.IP
.nf
\f[C]
[[1.46990588e\-19\ 1.00000000e+00]
\ [7.23344224e\-10\ 9.99999999e\-01]
\ [8.43160984e\-10\ 9.99999999e\-01]
\ ...
\ [4.03499383e\-04\ 9.99596501e\-01]
\ [3.03132738e\-13\ 1.00000000e+00]
\ [6.14030540e\-03\ 9.93859695e\-01]]
\f[]
.fi
.PP
Like in fit(), frovedis\-like input can be used to speed\-up the
prediction making on the trained model at server side.
.PP
For example,
.IP
.nf
\f[C]
#\ Since\ "cmat"\ is\ FrovedisColmajorMatrix,\ we\ have\ created\ FrovedisRowmajorMatrix.
from\ frovedis.matrix.dense\ import\ FrovedisRowmajorMatrix

#\ finds\ the\ probablity\ sample\ for\ each\ class\ in\ the\ model
lr.predict_proba(mat)\ \ 
\f[]
.fi
.PP
Output
.IP
.nf
\f[C]
[[1.46990588e\-19\ 1.00000000e+00]
\ [7.23344224e\-10\ 9.99999999e\-01]
\ [8.43160984e\-10\ 9.99999999e\-01]
\ ...
\ [4.03499383e\-04\ 9.99596501e\-01]
\ [3.03132738e\-13\ 1.00000000e+00]
\ [6.14030540e\-03\ 9.93859695e\-01]]
\f[]
.fi
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns a numpy array of float or double(float64) type and of shape
(n_samples, n_classes) containing the prediction probability values.
.SS 5. score(X, y, sample_weight = None)
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]X\f[]\f[]: A numpy dense or scipy sparse matrix or any python
array\-like object or an instance of FrovedisCRSMatrix for sparse data
and FrovedisRowmajorMatrix for dense data.
.PD 0
.P
.PD
\f[B]\f[I]y\f[]\f[]: Any python array\-like object or an instance of
FrovedisDvector.
.PD 0
.P
.PD
\f[B]\f[I]sample_weight\f[]\f[]: Python array\-like containing the
intended weights for each input samples and it should be the shape of
(nsamples, ).
When it is None (not specified explicitly), an uniform weight vector is
assigned on each input sample.
(Default: None)
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
Calculate mean accuracy on the given test data and labels i.e.
mean accuracy of self.predict(X) wrt.
y.
.PP
For example,
.IP
.nf
\f[C]
#\ calculate\ mean\ accuracy\ score\ on\ given\ test\ data\ and\ labels
lr.score(mat,lbl)\ \ 
\f[]
.fi
.PP
Output
.IP
.nf
\f[C]
0.96
\f[]
.fi
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns an accuracy score of float type.
.SS 6. load(fname, dtype = None)
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]fname\f[]\f[]: A string object containing the name of the file
having model information to be loaded.
.PD 0
.P
.PD
\f[B]\f[I]dtype\f[]\f[]: A data\-type is inferred from the input data.
Currently, expected input data\-type is either float or double(float64).
(Default: None)
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It loads the model from the specified file (having little\-endian binary
data).
.PP
For example,
.IP
.nf
\f[C]
lr.load("./out/LRModel")
\f[]
.fi
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It simply returns "self" instance.
.SS 7. save(fname)
.PP
\f[B]Parameters\f[]
.PD 0
.P
.PD
\f[B]\f[I]fname\f[]\f[]: A string object containing the name of the file
on which the target model is to be saved.
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
On success, it writes the model information (weight values etc.) in the
specified file as little\-endian binary data.
Otherwise, it throws an exception.
.PP
For example,
.IP
.nf
\f[C]
#\ To\ save\ the\ logistic\ regression\ model
lr.save("./out/LRModel")\ \ \ \ 
\f[]
.fi
.PP
This will save the logistic regression model on the path
\[aq]/out/LRModel\[aq].
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns nothing.
.SS 8. debug_print()
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It shows the target model information (weight values, intercept, etc.)
on the server side user terminal.
It is mainly used for debugging purpose.
.PP
For example,
.IP
.nf
\f[C]
lr.debug_print()\ \ 
\f[]
.fi
.PP
Output
.IP
.nf
\f[C]
\-\-\-\-\-\-\-\-\ Weight\ Vector::\ \-\-\-\-\-\-\-\-
25.4745\ 47.8416\ 155.732\ 190.863\ 0.271114\ 0.0911008\ \-0.151433\ \-0.0785512\ 0.511576
0.203452\ 0.199293\ 3.8659\ 1.22203\ \-42.3556\ 0.0239707\ 0.0395711\ 0.0389786\ 0.017432
0.0647208\ 0.0105295\ 24.7162\ 60.7113\ 150.789\ \-148.921\ 0.354222\ 0.104251\ \-0.202345
\-0.0363726\ 0.734499\ 0.22635
Intercept::\ 60.7742
Threshold::\ 0.5
\f[]
.fi
.PP
It displays the weights, intercept, etc.
values on the trained model which is currently present on the server.
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns nothing.
.SS 9. release()
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It can be used to release the in\-memory model at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
lr.release()
\f[]
.fi
.PP
This will reset the after\-fit populated attributes to None, along with
releasing server side memory.
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns nothing.
.SS 10. is_fitted()
.PP
\f[B]Purpose\f[]
.PD 0
.P
.PD
It can be used to confirm if the model is already fitted or not.
In case, predict() is used before training the model, then it can prompt
the user to train the logistic regression model first.
.PP
\f[B]Return Value\f[]
.PD 0
.P
.PD
It returns ‘True', if the model is already fitted otherwise, it returns
‘False'.
.SH SEE ALSO
.PP
linear_svm, dvector, crs_matrix
