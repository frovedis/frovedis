.\" Automatically generated by Pandoc 2.17.1.1
.\"
.\" Define V font for inline verbatim, using C font in formats
.\" that render this, and otherwise B font.
.ie "\f[CB]x\f[]"x" \{\
. ftr V B
. ftr VI BI
. ftr VB B
. ftr VBI BI
.\}
.el \{\
. ftr V CR
. ftr VI CI
. ftr VB CB
. ftr VBI CBI
.\}
.TH "FPGrowth" "" "" "" ""
.hy
.SH NAME
.PP
FPGrowth - A frequent pattern mining algorithm supported by Frovedis.
.SH SYNOPSIS
.IP
.nf
\f[C]
class frovedis.mllib.fpm.FPGrowth(minSupport=0.3, minConfidence=0.8, itemsCol=\[aq]items\[aq],  
                                  predictionCol=\[aq]prediction\[aq], numPartitions=None,  
                                  tree_depth=None, compression_point=4, mem_opt_level=0,  
                                  verbose=0, encode_string_input=False)  
\f[R]
.fi
.SS Public Member Functions
.PP
fit(data)
.PD 0
.P
.PD
generate_rules(confidence = None)
.PD 0
.P
.PD
transform(data)
.PD 0
.P
.PD
load(fname)
.PD 0
.P
.PD
save(fname)
.PD 0
.P
.PD
debug_print()
.PD 0
.P
.PD
release()
.PD 0
.P
.PD
is_fitted()
.SH DESCRIPTION
.PP
FPGrowth is an algorithm for discovering frequent itemsets in a
transaction database.
The input of FPGrowth is a set of transactions called transaction
database.
Each transaction is a set of items.
\f[B]Frovedis supports numeric and non-numeric values for transaction
data.\f[R]
.PP
For example, consider the following transaction database.
It contains 4 transactions (t1, t2, t3, t4) and 4 items (1, 2, 3, 4).
The first transaction represents the set of items 1, 2 , 3 and 4.
.IP
.nf
\f[C]
Transaction id      Items  
t1                  {1, 2, 3, 4}  
t2                  {1, 2, 3}  
t3                  {1, 2}  
t4                  {1}  
\f[R]
.fi
.PP
It is important to note that repetition of item in any given transaction
needs to be avoided.
It would raise exception in that case.
.PP
Now, if FPGrowth is run on the above transaction database with a
minSupport of 40% and a tree_depth of 5 levels,
.PP
FPGrowth produces the following result:
.IP
.nf
\f[C]
items             freq
[1]               4
[2]               3
[3]               2
[2, 1]            3
[3, 1]            2
[3, 2]            2
[3, 2, 1]         2
\f[R]
.fi
.PP
In the results, each itemset is annotated with its corresponding
frequency.
.PP
This module provides a client-server implementation, where the client
application is a normal python program.
In this implementation, a python client can interact with a frovedis
server by sending the required python data for training at frovedis
side.
Python data is converted into frovedis compatible data internally and
the python ML call is linked with the respective frovedis ML call to get
the job done at frovedis server.
.PP
Python side calls for FPGrowth on the frovedis server.
Once the training is completed with the input data at the frovedis
server, it returns an abstract model with a unique model ID to the
client python program.
.PP
When prediction-like request would be made on the trained model, python
program will send the same request to the frovedis server.
After the request is served at the frovedis server, the output would be
sent back to the python client.
.SS Detailed Description
.SS 1. FPGrowth()
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]minSupport\f[B]\f[R]: A positive double (float64) type value
that specifies the minimum support level of frequent itemsets.
Its value must be within 0 to 1.
(Default: 0.3)
.PD 0
.P
.PD
\f[B]\f[BI]minConfidence\f[B]\f[R]: A positive double (float64) type
value that specifies the minimal confidence for generating association
rules.
It will not affect the mining for frequent itemsets, but will affect the
association rules generation.
Its value must be within 0 to 1.
(Default: 0.8)
.PD 0
.P
.PD
\f[B]\f[BI]itemsCol\f[B]\f[R]: An unsed parameter.
(Default: `items')
.PD 0
.P
.PD
\f[B]\f[BI]predictionCol\f[B]\f[R]: An unsed parameter.
(Default: `prediction')
.PD 0
.P
.PD
\f[B]\f[BI]numPartitions\f[B]\f[R]: An unsed parameter.
(Default: None)
.PD 0
.P
.PD
\f[B]\f[BI]tree_depth\f[B]\f[R]: A positive integer parameter specifying
the maximum number of levels for tree construction.
Its value must be greater than 1.
(Default: None)
.PD 0
.P
.PD
When it is None (not specified explicitly), the tree is constructed to
its maximum depth according to the data.
Since transaction databases tend to be very large, there may be a
scenario wherein entire FP tree cannot be contained in memory.
In those cases, the size of FP tree may be limited by using this
parameter.
.PD 0
.P
.PD
\f[B]\f[BI]compression_point\f[B]\f[R]: A positive integer parameter.
This is an internal memory optimisation strategy which helps when
working with large transaction databases.
Its value must be greater than or equal to 2.
No compression will be performed till the level specified by this
parameter is reached.
(Default: 4)
.PD 0
.P
.PD
\f[B]\f[BI]mem_opt_level\f[B]\f[R]: An integer value which must be
either 0 (memory optimisation OFF) or 1 (memory optimisation ON).
If switched On, it will lead to removal of redundant tree data residing
in memory at server side.
It should only be used where systems have memory constraints.
By default, it is 0, but in case of memory constraints, execution should
be attempted keeping this value as 1, it will help in reducing memory
footprint.
However, when it is 1, it might still cause memory issue in case data is
too big.
In this case, data may be spilled onto disk if this environment variable
has been set.
This will degrade performance (execution time).
.PD 0
.P
.PD
\f[B]\f[BI]verbose\f[B]\f[R]: An integer parameter specifying the log
level to use.
Its value is 0 by default (INFO level).
But it can be set to 1 (DEBUG level) or 2 (TRACE level) for getting
training time logs from frovedis server.
.PD 0
.P
.PD
\f[B]\f[BI]encode_string_input\f[B]\f[R]: A boolean parameter when set
to True, encodes the non-numeric (like strings) itemset values.
It first internally encodes the named-items to an encoded numbered-items
and the encoded dataframe is used for further training at frovedis
server.
It helps to train the encoded fpgrowth model faster with non-numeric
itemsets since data present with server is two column dataframe,
operations like join(), etc are little on slower on non-numeric columns
than the numeric columns.
(Default: False)
.PP
For example,
.IP
.nf
\f[C]
# let the data be some non-numeric transaction database 
data = [[\[aq]banana\[aq],\[aq]apple\[aq],\[aq]mango\[aq],\[aq]cake\[aq]],
        [\[aq]cake\[aq],\[aq]banana\[aq],\[aq]apple\[aq]],
        [\[aq]bread\[aq],\[aq]banana\[aq]],
        [\[aq]banana\[aq]]]
# Using FPGrowth object with memory optimization parameters for training 
# Here, disabling the parameter encode_string_input = False by default
from frovedis.mllib.fpm import FPGrowth
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1) 
fpm.fit(data)
\f[R]
.fi
.PP
\f[B]Frequent itemsets generation time: 0.0361 sec\f[R]
.PP
And, when enabling `encode_string_input' with the same non-numeric data,
.IP
.nf
\f[C]
# Using FPGrowth object with memory optimization parameters for training 
# Here, parameter encode_string_input = True 
from frovedis.mllib.fpm import FPGrowth
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1, encode_string_input = True) 
fpm.fit(data)
\f[R]
.fi
.PP
\f[B]Frequent itemsets generation time: 0.0315 sec\f[R]
.PP
\f[B]Attributes\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]freqItemsets\f[B]\f[R]: A pandas dataframe having two fields,
`items' and `freq', where `items' is an array whereas `freq' is double
(float64) type value.
It contains the itemsets along with their frequency values.
Here, the frequency of an itemset signifies as to how many times the
itemset appears in the transaction database.
.PD 0
.P
.PD
\f[B]\f[BI]associationRules\f[B]\f[R]: A pandas dataframe having six
fields, `antecedent', `consequent', `confidence', `lift', `support' and
`conviction'.
.PD 0
.P
.PD
Every association rule is composed of two parts: an antecedent (if) and
a consequent (then).
.PD 0
.P
.PD
An `antecedent' is an item found within the data.
A `consequent' is an item found in combination with the `antecedent'.
Both are itemsets (arrays).
.PD 0
.P
.PD
For measuring the effectiveness of association rule, `confidence',
`lift', `support' and `conviction' are used.
All are double (float64) type values.
.PD 0
.P
.PD
`confidence' refers to the amount of times a given rule turns out to be
true in practice.
.PD 0
.P
.PD
`support' is an indication of how frequently the itemset appears in the
dataset.
.PD 0
.P
.PD
`lift' is the ratio of confidence to support.
If the rule has a lift of 1, it would imply that the probability of
occurrence of the `antecedent' and that of the `consequent' are
independent of each other.
When two events are independent of each other, no rule can be drawn
involving those two events.
If the lift is greater than 1, that lets us know the degree to which
those two occurrences are dependent on one another, and makes those
rules potentially useful for predicting the consequent in future data
sets.
If the lift is less than 1, that lets us know the items are substitute
to each other.
This means that presence of one item has negative effect on presence of
other item and vice versa.
.PD 0
.P
.PD
`conviction' compares the probability that X appears without Y if they
were dependent with the actual frequency of the appearance of X without
Y.
If it equals 1, then they are completely unrelated.
.PP
\f[B]\f[BI]count\f[B]\f[R]: A positive integer value which specifies the
frequent itemsets count.
.PD 0
.P
.PD
\f[B]\f[BI]encode_logic\f[B]\f[R]: A python dictionary having
transaction items with a corresponding encoded number as key-value
pairs.
It is only available when transaction items are have string values and
`encode_string_input' parameter is set to True.
Otherwise it is None.
This is used internally to perform auto decoding of items during
`freqItemsets' construction.
.PP
For example,
.IP
.nf
\f[C]
# let data be some non-numeric transaction database 
data = [[\[aq]banana\[aq],\[aq]apple\[aq],\[aq]mango\[aq],\[aq]cake\[aq]],
        [\[aq]cake\[aq],\[aq]banana\[aq],\[aq]apple\[aq]],
        [\[aq]bread\[aq],\[aq]banana\[aq]],
        [\[aq]banana\[aq]]]

# creating a pandas dataframe
import pandas as pd
dataDF = pd.DataFrame(data)

# Using FPGrowth object with memory optimization parameters and enabling encoding on non-numeric 
# inputs and training 
from frovedis.mllib.fpm import FPGrowth
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1, encode_string_input = True) 
fpm.fit(data)
print(\[dq]logic: \[dq], fpm.encode_logic)
\f[R]
.fi
.PP
Output,
.IP
.nf
\f[C]
logic:  {1: \[aq]apple\[aq], 2: \[aq]banana\[aq], 3: \[aq]bread\[aq], 4: \[aq]cake\[aq], 5: \[aq]mango\[aq]}
\f[R]
.fi
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It initializes an FPGrowth object with the given parameters.
.PP
The parameters: \[lq]itemsCol\[rq], \[lq]predictionCol\[rq] and
\[lq]numPartitions\[rq] are simply kept in to make the interface uniform
to the PySpark FPGrowth module.
They are not used anywhere within the frovedis implementation.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 2. fit(data)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]data\f[B]\f[R]: A python iterable or a pandas dataframe
(manually constructed or loaded from file) or frovedis-two column
dataframe containing the transaction data.
Frovedis supports numeric and non-numeric values in transaction dataset.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It accepts the training data and trains the fpgrowth model with
specified minimum support value and tree depth value for construction of
tree.
.PP
For pandas dataframe, if it has 2 columns, the second column would be
treated as items and needs to be an array-like input.
.PP
For example,
.IP
.nf
\f[C]
# creating a pandas dataframe 
import pandas as pd
data = [[\[aq]banana\[aq],\[aq]apple\[aq],\[aq]mango\[aq],\[aq]cake\[aq]], 
        [\[aq]cake\[aq],\[aq]banana\[aq],\[aq]apple\[aq]], 
        [\[aq]bread\[aq],\[aq]banana\[aq]], 
        [\[aq]banana\[aq]]]
dataDF = pd.DataFrame(data)

# fitting input dataframe on FPGrowth object  
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1).fit(dataDF)   
# Now, to print the frequent itemsets table after training is completed
print(fpm.freqItemsets)   
# to print table with all the association rules 
print(fpm.associationRules)
\f[R]
.fi
.PP
Output,
.IP
.nf
\f[C]
frequent itemsets:
                           items  freq
0                       [banana]   4.0
1                        [apple]   2.0
2                         [cake]   2.0
3                        [bread]   1.0
4                        [mango]   1.0
5                  [cake, apple]   2.0
6                 [mango, apple]   1.0
7                [apple, banana]   2.0
8                [bread, banana]   1.0
9                 [cake, banana]   2.0
10               [mango, banana]   1.0
\&...
16  [mango, cake, apple, banana]   1.0
association rules:
                antecedent consequent  confidence  lift  support  conviction
0                 [banana]    [apple]         0.5   1.0     0.50         1.0
1                  [apple]     [cake]         1.0   2.0     0.50         NaN
2                 [banana]     [cake]         0.5   1.0     0.50         1.0
3                  [apple]    [mango]         0.5   2.0     0.25         1.5
4                   [cake]    [mango]         0.5   2.0     0.25         1.5
5                  [apple]   [banana]         1.0   1.0     0.50         NaN
6                  [bread]   [banana]         1.0   1.0     0.25         NaN
7                   [cake]    [apple]         1.0   2.0     0.50         NaN
8                   [cake]   [banana]         1.0   1.0     0.50         NaN
9                  [mango]    [apple]         1.0   2.0     0.25         NaN
10                 [mango]   [banana]         1.0   1.0     0.25         NaN
\&...
27    [mango, cake, apple]   [banana]         1.0   1.0     0.25         NaN
\f[R]
.fi
.PP
If it has more than 2 columns, all columns would be treated as
individual item and missing items in any given transaction needs to be
NaN.
.PP
For example,
.PP
FILE: groceries.csv
.PP
Item(s),Item 1,Item 2,Item 3,Item 4,Item 5,Item 6,Item 7, \&....
, Item 30,Item 31,Item 32
.PD 0
.P
.PD
4,citrus fruit,semi-finished bread,margarine,ready
soups,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.PD 0
.P
.PD
3,tropical fruit,yogurt,coffee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.PD 0
.P
.PD
1,whole milk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.PD 0
.P
.PD
4,pip fruit,yogurt,cream cheese,meat spreads,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.PD 0
.P
.PD
\&...
.PD 0
.P
.PD
10,chicken,citrus fruit,other vegetables,butter,yogurt, \&..., cling
film/bags,,,,,,,,,,,,,,,,,,,,,,
.PD 0
.P
.PD
4,semi-finished bread,bottled water,soda,bottled
beer,,,,,,,,,,,,,,,,,,,,,,,,,,,,
.PD 0
.P
.PD
5,chicken,tropical fruit,other vegetables,vinegar,shopping
bags,,,,,,,,,,,,,,,,,,,,,,,,,,,
.IP
.nf
\f[C]
# reading data from csv file 
import pandas as pd
dataDF = pd.read_csv(\[dq]./input/groceries.csv\[dq], dtype=str).drop([\[aq]Item(s)\[aq]], axis=1)

# fitting input dataframe on FPGrowth object  
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1).fit(dataDF)   
# Now, to print the frequent itemsets table after training is completed
print(fpm.freqItemsets)   
# to print table with all the association rules 
print(fpm.associationRules)
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
frequent itemsets:
                                        items    freq
0                                [whole milk]  2513.0
1                          [other vegetables]  1903.0
2                                [rolls/buns]  1809.0
3                                      [soda]  1715.0
4                                    [yogurt]  1372.0
\&..                                        ...     ...
328  [whipped/sour cream, yogurt, whole milk]   107.0
329    [yogurt, other vegetables, whole milk]   219.0
330    [yogurt, rolls/buns, other vegetables]   113.0
331          [yogurt, rolls/buns, whole milk]   153.0
332                [yogurt, soda, whole milk]   103.0
association rules:
                          antecedent         consequent    confidence    lift   support  conviction
0             [yogurt, other vegetables]    [whole milk]     0.512881  2.007235 0.022267 1.528340
1           [whipped/sour cream, yogurt]    [whole milk]     0.524510  2.052747 0.010880 1.565719
2 [whipped/sour cream, other vegetables]    [whole milk]     0.507042  1.984385 0.014642 1.510239
3               [tropical fruit, yogurt]    [whole milk]     0.517361  2.024770 0.015150 1.542528
4      [tropical fruit, root vegetables]    [whole milk]     0.570048  2.230969 0.011998 1.731553
5              [root vegetables, yogurt]    [whole milk]     0.562992  2.203354 0.014540 1.703594
6          [root vegetables, rolls/buns]    [whole milk]     0.523013  2.046888 0.012710 1.560804
7          [pip fruit, other vegetables]    [whole milk]     0.517510  2.025351 0.013523 1.543003
8      [domestic eggs, other vegetables]    [whole milk]     0.552511  2.162336 0.012303 1.663694
9                         [curd, yogurt]    [whole milk]     0.582353  2.279125 0.010066 1.782567
10            [butter, other vegetables]    [whole milk]     0.573604  2.244885 0.011490 1.745992
11     [tropical fruit, root vegetables] [other vegetables]  0.584541  3.020999 0.012303 1.941244
12             [root vegetables, yogurt] [other vegetables]  0.500000  2.584078 0.012913 1.613015
13         [root vegetables, rolls/buns] [other vegetables]  0.502092  2.594890 0.012201 1.619792
14       [citrus fruit, root vegetables] [other vegetables]  0.586207  3.029608 0.010371 1.949059
\f[R]
.fi
.PP
When native python iterable or a pandas dataframe is provided, it is
converted to frovedis dataframe and sent to frovedis server which
consumes some data transfer time.
Pre-constructed frovedis-like inputs can be used to speed up the
training time, especially when same data would be used for multiple
executions.
The storage representation when creating frovedis dataframe is slightly
different than pandas dataframe as shown below :
.IP
.nf
\f[C]
# creating a pandas dataframe
import pandas as pd
data = [[\[aq]banana\[aq],\[aq]apple\[aq],\[aq]mango\[aq],\[aq]cake\[aq]], 
        [\[aq]cake\[aq],\[aq]banana\[aq],\[aq]apple\[aq]], 
        [\[aq]bread\[aq],\[aq]banana\[aq]], 
        [\[aq]banana\[aq]]]
dataDF = pd.DataFrame(data)

# Creating a two column frovedis dataframe
import frovedis.dataframe as fpd
frovDF = fpd.DataFrame(dataDF)

# fitting frovedis-like input dataframe on FPGrowth object  
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1).fit(frovDF)  
# Now, to print the frequent itemsets table after training is completed
print(fpm.freqItemsets)   
# to print table with all the association rules 
print(fpm.associationRules)
\f[R]
.fi
.PP
Output,
.IP
.nf
\f[C]
frequent itemsets:
                           items  freq
0                       [banana]   4.0
1                        [apple]   2.0
2                         [cake]   2.0
3                        [bread]   1.0
4                        [mango]   1.0
5                  [cake, apple]   2.0
6                 [mango, apple]   1.0
7                [apple, banana]   2.0
8                [bread, banana]   1.0
9                 [cake, banana]   2.0
10               [mango, banana]   1.0
\&...
16  [mango, cake, apple, banana]   1.0
association rules:
                antecedent consequent  confidence  lift  support  conviction
0                 [banana]    [apple]         0.5   1.0     0.50         1.0
1                  [apple]     [cake]         1.0   2.0     0.50         NaN
2                 [banana]     [cake]         0.5   1.0     0.50         1.0
3                  [apple]    [mango]         0.5   2.0     0.25         1.5
4                   [cake]    [mango]         0.5   2.0     0.25         1.5
5                  [apple]   [banana]         1.0   1.0     0.50         NaN
6                  [bread]   [banana]         1.0   1.0     0.25         NaN
7                   [cake]    [apple]         1.0   2.0     0.50         NaN
8                   [cake]   [banana]         1.0   1.0     0.50         NaN
9                  [mango]    [apple]         1.0   2.0     0.25         NaN
10                 [mango]   [banana]         1.0   1.0     0.25         NaN
\&...
27    [mango, cake, apple]   [banana]         1.0   1.0     0.25         NaN
\f[R]
.fi
.PP
Another way to work with frovedis dataframe is by loading it from a
file.
Since in real world, large transaction database will be used for rule
mining, such files needs to have two columns named \[lq]trans_id\[rq]
and \[lq]item\[rq], where \[lq]trans_id\[rq] column will have respective
transaction id and \[lq]item\[rq] column will have the individual items
in that transaction.
If other names are present for the columns in the file, then it would
raise exception.
.PP
For example:
.PP
FILE: trans.csv
.PP
trans_id,item 1,1
.PD 0
.P
.PD
1,2
.PD 0
.P
.PD
1,3
.PD 0
.P
.PD
1,4
.PD 0
.P
.PD
2,1
.PD 0
.P
.PD
2,2
.PD 0
.P
.PD
2,3
.PD 0
.P
.PD
3,1
.PD 0
.P
.PD
3,2
.PD 0
.P
.PD
4,1
.PP
The above data can be loaded and passed to frovedis FPGrowth as follows:
.IP
.nf
\f[C]
# loadinf frovedis dataframe from a csv file
import frovedis.dataframe as fdf
frovDF = fdf.read_csv(\[lq]trans.csv\[rq])

# fitting frovedis-like input dataframe on FPGrowth object  
fpm = FPGrowth(minSupport = 0.01, minConfidence = 0.5, compression_point = 4, 
               mem_opt_level = 1).fit(frovDF)  
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 3. generate_rules(confidence = None)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]confidence\f[B]\f[R]: A double (float64) type parameter
indicating the minimum confidence value.
(Default: None)
.PD 0
.P
.PD
When it is None (not specified explicitly), then it will use confidence
value used during FPGrowth object creation.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It accepts the minimum confidence value to trim the rules during the
generation of association rules.
.PP
For example,
.IP
.nf
\f[C]
# generating rules with minimum confidence value of 0.2  
fp_rules = fpm.generate_rules(0.2)  
\f[R]
.fi
.PP
This will generate tables containing rules at server side.
.PD 0
.P
.PD
To print these generated rules, debug_print() may be used as show below:
.IP
.nf
\f[C]
fp_rules.debug_print()
\f[R]
.fi
.PP
This will show all tables of different antecedent length at server side.
Here, encoding was disabled during rule generation.
.PP
Output,
.IP
.nf
\f[C]
--- rule[0] ---
antecedent1   consequent   confidence   lift   support  conviction
      apple   cake         1            2      0.5      NULL
      apple   mango        0.5          2      0.25     1.5
     banana   apple        0.5          1      0.5      1
     banana   cake         0.5          1      0.5      1
       cake   mango        0.5          2      0.25     1.5

--- rule[1] ---
antecedent1   consequent   confidence   lift   support  conviction
       cake   apple        1            2      0.5      NULL
      mango   apple        1            2      0.25     NULL
      apple   banana       1            1      0.5      NULL
      bread   banana       1            1      0.25     NULL
       cake   banana       1            1      0.5      NULL
      mango   banana       1            1      0.25     NULL
      mango   cake         1            2      0.25     NULL

--- rule[2] ---
antecedent1   antecedent2  consequent  confidence  lift  support  conviction
      apple   banana       cake        1           2     0.5      NULL
       cake   banana       mango       0.5         2     0.25     1.5
      apple   banana       mango       0.5         2     0.25     1.5
       cake   apple        mango       0.5         2     0.25     1.5

--- rule[3] ---
antecedent1   antecedent2  consequent  confidence  lift  support  conviction
      mango   banana       apple       1           2     0.25     NULL
       cake   banana       apple       1           2     0.5      NULL
      mango   banana       cake        1           2     0.25     NULL
      mango   apple        cake        1           2     0.25     NULL

--- rule[4] ---
antecedent1   antecedent2  consequent  confidence  lift  support  conviction
      mango   cake         banana      1           1     0.25     NULL
      mango   apple        banana      1           1     0.25     NULL
       cake   apple        banana      1           1     0.5      NULL
      mango   cake         apple       1           2     0.25     NULL

--- rule[5] ---
antecedent1   antecedent2  antecedent3 consequent  confidence  lift  support  conviction
       cake   apple        banana           mango  0.5         2     0.25     1.5

--- rule[6] ---
antecedent1   antecedent2  antecedent3 consequent  confidence  lift  support  conviction
      mango   apple        banana          cake    1           2     0.25     NULL

--- rule[7] ---
antecedent1   antecedent2  antecedent3 consequent  confidence  lift  support  conviction
      mango   cake         banana          apple   1           2     0.25     NULL

--- rule[8] ---
antecedent1   antecedent2  antecedent3 consequent  confidence  lift  support  conviction
      mango   cake         apple           banana  1           1     0.25     NULL
\f[R]
.fi
.PP
\f[B]This output will be visible on server side.
No such output will be visible on client side.\f[R]
.PP
These above generated rules can also be saved and loaded separately as
shown below:
.IP
.nf
\f[C]
rule.save(\[dq]./out/FPRule\[dq])
\f[R]
.fi
.PP
It saves the rules in `FPRule' directory.
.PD 0
.P
.PD
It would raise exception if the directory already exists with same name.
.PP
The `FPRule' directory has
.PP
\f[B]FPRule\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] encode_logic
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_0\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_1\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_2\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_3\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_4\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_5\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_6\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_7\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]rule_8\f[R]
.PP
The `encode_logic' file is created only when `encode_string_input =
True' while training.
.PD 0
.P
.PD
Other directories are created according to number of rules created which
were constructed during training.
.PD 0
.P
.PD
Each rule based directory contains information about antecedent,
consequent, confidence, lift, support, conviction.
.PP
For loading the already saved rules, following should be done:
.IP
.nf
\f[C]
rule.load(\[dq]./out/FPRule\[dq])
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns an FPRules object.
.SS 4. transform(data)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]data\f[B]\f[R]: A python iterable or a pandas dataframe or
frovedis-like dataframe containing the transaction data.
Frovedis supports numeric and non-numeric values as for transaction
data.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It gives the prediction of list of items for each of the corresponding
transaction items.
.PP
For example,
.IP
.nf
\f[C]
print(fpm.transform(data))
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
                          items     prediction
0  [banana, apple, mango, cake]             []
1         [cake, banana, apple]        [mango]
2               [bread, banana]  [apple, cake]
3                      [banana]  [apple, cake]
\f[R]
.fi
.PP
In case no prediction is made for any list of items, then it will give
an empty list, just like in pandas dataframe as well.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns a pandas dataframe having two fields `items' and
`prediction'.
Here `items' is a list of transaction items and `prediction' is a
corresponding list of predictions for these transaction items.
.SS 5. load(fname)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fname\f[B]\f[R]: A string object containing the name of the
file having model information to be loaded.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It loads the model from the specified file path.
.PP
For example,
.IP
.nf
\f[C]
fpm.load(\[dq]./out/FPModel\[dq])  
\f[R]
.fi
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It simply returns \[lq]self\[rq] reference.
.SS 6. save(fname)
.PP
\f[B]Parameters\f[R]
.PD 0
.P
.PD
\f[B]\f[BI]fname\f[B]\f[R]: A string object containing the name of the
file on which the target model is to be saved.
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
On success, it writes the model information in the specified file path.
Otherwise, it throws an exception.
.PP
For example,
.IP
.nf
\f[C]
# To save the FPGrowth model
fpm.save(\[dq]./out/FPModel\[dq])  
\f[R]
.fi
.PP
This will save the fp-growth model on the path `/out/FPModel'.
.PD 0
.P
.PD
It would raise exception if the directory already exists with same name.
.PP
The `FPModel' directory has
.PP
\f[B]FPModel\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] encode_logic
.PD 0
.P
.PD
|\[em]\[em]\[en] metadata
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]tree_0\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]tree_1\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]tree_2\f[R]
.PD 0
.P
.PD
|\[em]\[em]\[en] \f[B]tree_3\f[R]
.PP
The `encode_logic' file is created only when `encode_string_input =
True' while training.
.PD 0
.P
.PD
The metadata file contains the number of transactions and rule frequent
itemsets count.
.PD 0
.P
.PD
Rest of the directories are created according to tree levels which were
constructed during training.
.PD 0
.P
.PD
Each tree based directory contains information about items and their
frequency count on each tree level.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing
.SS 7. debug_print()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It shows the target model information (frequent itemsets, generated
fprules, frequent itemsets count) on the server side user terminal.
It is mainly used for debugging purpose.
.PP
For example,
.IP
.nf
\f[C]
fpm.debug_print()  
\f[R]
.fi
.PP
Output
.IP
.nf
\f[C]
--- item_support ---
item    item_support
banana  1
apple   0.5
cake    0.5
bread   0.25
mango   0.25

--- tree[0] ---
item    count
banana  4
apple   2
cake    2
bread   1
mango   1

--- tree_info[0] ---

--- tree[1] ---
item    item1   count
cake    apple   2
mango   apple   1
apple   banana  2
bread   banana  1
cake    banana  2
mango   banana  1
mango   cake    1

--- tree_info[1] ---

--- tree[2] ---
item    item1   item2   count
mango   cake    apple   1
cake    apple   banana  2
mango   apple   banana  1
mango   cake    banana  1

--- tree_info[2] ---

--- tree[3] ---
item    item1   item2   item3   count
mango   cake    apple   banana  1

--- tree_info[3] ---

total #FIS: 17
\f[R]
.fi
.PP
This output will be visible on server side.
It displays the in memory frequent itemsets, generated fprules, frequent
itemsets count which is currently present on the server.
.PP
\f[B]No such output will be visible on client side.\f[R]
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing
.SS 8. release()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It can be used to release the in-memory model at frovedis server.
.PP
For example,
.IP
.nf
\f[C]
fpm.release()  
\f[R]
.fi
.PP
This will reset the after-fit populated attributes to None, along with
releasing server side memory.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns nothing
.SS 9. is_fitted()
.PP
\f[B]Purpose\f[R]
.PD 0
.P
.PD
It can be used to confirm if the model is already fitted or not.
.PP
\f[B]Return Value\f[R]
.PD 0
.P
.PD
It returns `True', if the model is already fitted, otherwise, it returns
`False'.
.SH SEE ALSO
.IP \[bu] 2
\f[B]Introduction to frovedis DataFrame\f[R]
